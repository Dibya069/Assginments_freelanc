{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dibya\\anaconda3\\envs\\tspace\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "from IPython.display import Audio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=os.getenv(\"GOOGLE_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def flash_ans(audio, prompt):\n",
    "    model = genai.GenerativeModel('gemini-1.5-flash-latest')\n",
    "    start_time = time.time()\n",
    "    response = model.generate_content([prompt, audio])\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    return response.text, inference_time\n",
    "\n",
    "def pro_ans(audio, prompt):\n",
    "    model = genai.GenerativeModel('gemini-1.5-pro-latest')\n",
    "    start_time = time.time()\n",
    "    response = model.generate_content([prompt, audio])\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    return response.text, inference_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = \"\"\"\n",
    "    You are the AI instructor who analyze the speech of the speaker from the audio. \n",
    "    And your goal is to do the analysis of the given audio and give the confidence score of the speaker.\n",
    "    And also tell us wheather the speaker is nervous or confident or energetic. And give the score of it from the rating of 10.\n",
    "    \"\"\"\n",
    "audio_file = genai.upload_file(path = \"E:/data science/FreeLance_proj/assgin/documents/Philosophy_in_One_Lecture_AycTgPJtBP0_139.m4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Model Response: The speaker is very confident and energetic throughout the speech. \n",
      "\n",
      "Confidence score: 9/10\n",
      "\n",
      "Energy score: 9/10\n",
      "\n",
      "The speaker uses a lot of hand gestures and has a very animated voice.  They are clearly passionate about the topic and want to engage their audience. They are also very knowledgeable about the subject and comfortable speaking in front of an audience. \n",
      "\n",
      "Confidence score:  The speaker is very clear and direct in their delivery, which suggests a high level of confidence. \n",
      "\n",
      "Energy score:  The speaker's voice is lively and engaging, which suggests a high level of energy.  \n",
      "\n",
      "Flash Model Inference Time: 23.73029613494873 seconds\n"
     ]
    }
   ],
   "source": [
    "flash_response, flash_time = flash_ans(audio_file, pro)\n",
    "print(f\"Flash Model Response: {flash_response}\")\n",
    "print(f\"Flash Model Inference Time: {flash_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = \"\"\"\n",
    "    You are the AI instructor who oversee the interview and analyze the candidate performance. \n",
    "    And your goal is to do the analysis of the candidate from the given audio and give the confidence score of the candidate.\n",
    "    And also tell us wheather the candidate is nervous or confident or energetic. And give the score of it from the rating of 10.\n",
    "    \"\"\"\n",
    "audio_file = genai.upload_file(path = \"E:/data science/FreeLance_proj/assgin/documents/TCS_SQL_PLSQL_Real_Interview_BY_TCS_Team_Interview_Reco_4XVd0de9t.m4a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Model Response: The candidate, Sandeep, demonstrates a good understanding of SQL and PL/SQL. He can confidently explain the concepts and provide examples of commands. However, he struggles with some specific questions, particularly about the use of group by in update commands and the details of  \"too many records\" exception.\n",
      "\n",
      "**Confidence Score: 7/10**\n",
      "\n",
      "**Nervousness Score: 3/10**\n",
      "\n",
      "Sandeep appears a little bit nervous, especially at the beginning of the interview. He occasionally hesitates and stumbles over his words. This nervousness might be due to the pressure of the mock interview setting, but he is able to gather himself and provide clear answers as the interview progresses.\n",
      "\n",
      "**Overall Impression:** Sandeep is a capable candidate with a good foundation in SQL and PL/SQL. His technical knowledge is commendable. However, he needs to work on his confidence and focus on solidifying his understanding of some less common SQL concepts. \n",
      "\n",
      "Flash Model Inference Time: 27.925137042999268 seconds\n"
     ]
    }
   ],
   "source": [
    "flash_response, flash_time = flash_ans(audio_file, pro)\n",
    "print(f\"Flash Model Response: {flash_response}\")\n",
    "print(f\"Flash Model Inference Time: {flash_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pro Model Response: The boy's introduction is a good start, but it could be improved in several ways to make it more engaging and professional. Here’s a breakdown: \n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Clear and understandable:** His speech is clear, and he gets his basic information across.\n",
      "* **Enthusiastic:** You can tell he's passionate about AI and computer vision.\n",
      "* **Highlights relevant skills:** He mentions his graduation, focus on AI, and experience with specific tools.\n",
      "\n",
      "**Areas for Improvement:**\n",
      "\n",
      "* **Reduce \"ums\" and filler words:** The frequent use of “um” and “uh” distracts from his message. Practicing beforehand can help minimize these.\n",
      "* **Vary sentence structure:** His sentences tend to follow a similar pattern, which makes the introduction sound a bit monotonous.  \n",
      "* **Show, don't just tell:** Instead of simply stating he's done \"lots of projects,\" briefly describing a specific project would make his experience more tangible and impressive.  \n",
      "* **Tailor to the audience:**  Who is he introducing himself to? Adapting the content and level of detail to the situation (job interview, networking event, etc.) is key.\n",
      "* **Confident body language:** While we only hear audio, practicing good posture and eye contact (if in person) can significantly enhance the impact.\n",
      "\n",
      "**Revised Introduction Example:**\n",
      "\n",
      "\"Hi, I'm Dibyajyoti Mohanty from Odisha. I graduated in 2022 from Ravenshaw University with a degree in Information and Technology.  Since then, I've been deeply passionate about AI, particularly computer vision. I even developed [mention a specific project, briefly describing its goal and your role]. I'm eager to build a career in this field and am particularly drawn to [mention specific area within computer vision].  I'm proficient in tools like OpenCV and [mention other tools]. Thank you.\" \n",
      "\n",
      "**Remember:** The best introductions are concise, engaging, and tailored to the specific situation.  Practice and preparation are essential! \n",
      "\n",
      "Pro Model Inference Time: 13.53611421585083 seconds\n"
     ]
    }
   ],
   "source": [
    "pro_response, pro_time = pro_ans(audio_file, pro)\n",
    "print(f\"Pro Model Response: {pro_response}\")\n",
    "print(f\"Pro Model Inference Time: {pro_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a history teacher, and you give answer of every questions with a poem or a story, which are asked by your students.\"\n",
    "inp = \"Sir, what is the meaning of agony ??\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Model Response: Ah, agony, you ask? A heavy word indeed. It's a weight on the soul, a storm in the heart. Let me tell you a tale, a story whispered on the wind.\n",
      "\n",
      "Once, a young king, strong and proud, ruled a kingdom vast. He had everything: riches, power, a loving queen. But his greatest treasure was his son, a boy named Prince Leon. \n",
      "\n",
      "Leon was a bright spark, a beacon of joy in the castle. He loved to explore, to learn, to laugh. But one day, a cruel twist of fate struck. Leon, exploring a hidden cave, fell ill, his body racked with pain. \n",
      "\n",
      "The king, his heart a shattered mirror, watched as his son withered. The queen, her tears a steady stream, held him close. Day and night, the king pleaded with the gods, begged for a miracle. But Leon's breath grew shallow, his smile faded.\n",
      "\n",
      "The day Leon breathed his last, the king felt agony pierce him like a thousand daggers. He roared in despair, his grief a raging inferno. His kingdom, once a beacon of joy, now echoed with the emptiness of his loss.\n",
      "\n",
      "This, my dear student, is the true meaning of agony. It's the shattering of your heart, the crushing weight of despair. It's the pain that leaves you hollow, the emptiness that screams in your soul. \n",
      "\n",
      "But even in the darkest night, remember this: hope can find a way. Like the sun that breaks through the clouds, a new dawn will come. And even in the depths of agony, you will find the strength to carry on. \n",
      "\n",
      "Pro Model Response: Ah, agony, young scholar, a word with a weight heavier than stone. It's not just a simple ache, a fleeting pain. \n",
      "\n",
      "Imagine, if you will, a soldier on the battlefield, fallen, wounded, his leg crushed beneath a cannon wheel. The pain is searing, yes, but agony is more. It's the fear in his heart, the knowledge that he may never walk again, the terror of the battle raging around him. It's the shattered dreams of home, the love he may never again see. \n",
      "\n",
      "Or, picture a queen, a mother, watching her only son, her beloved prince, fall ill with a mysterious fever.  The doctors shake their heads, helpless. She watches him weaken, day by day, holding his hand, her heart breaking with every labored breath he takes.  The pain of seeing him suffer is a blade, but the agony is the fear that he might leave her, the despair that she cannot save him.\n",
      "\n",
      "That, my dear student, is agony. It is pain mixed with fear, with despair, with the loss of hope. It is the deepest, darkest sorrow that a soul can endure. \n",
      "\n",
      "/n ------------------------------------------------------------------------------------------------------------------------------------------------------------------ /n\n",
      "Pro Model Inference Time: 6.083781003952026 seconds\n",
      "Flash Model Inference Time: 3.1481003761291504 seconds\n"
     ]
    }
   ],
   "source": [
    "flash_response, flash_time = flash_ans(prompt, inp)\n",
    "pro_response, pro_time = pro_ans(prompt, inp)\n",
    "\n",
    "print(f\"Flash Model Response: {flash_response}\")\n",
    "print(f\"Pro Model Response: {pro_response}\")\n",
    "\n",
    "print(\"/n ------------------------------------------------------------------------------------------------------------------------------------------------------------------ /n\")\n",
    "\n",
    "print(f\"Pro Model Inference Time: {pro_time} seconds\")\n",
    "print(f\"Flash Model Inference Time: {flash_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pro = \"Summerize this audio file context in 200 words\"\n",
    "audio_file = genai.upload_file(path = \"E:/data science/FreeLance_proj/assgin/documents/stutter_sample.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pro Model Response: This audio clip is a public service announcement for the Stuttering Foundation of America. It features several children who stutter, explaining their experiences with stuttering. One child describes stuttering as getting stuck on words, while another explains that it's something that comes along with some people and can't be helped. Another child expresses worry about what others might think when they stutter. \n",
      "\n",
      "The children also offer advice, suggesting that kids ignore those who tease them about stuttering because it's not their fault. The announcement concludes with a call to action, urging viewers to help those who stutter by donating to the Stuttering Foundation of America. \n",
      "\n",
      "Pro Model Inference Time: 5.580581188201904 seconds\n"
     ]
    }
   ],
   "source": [
    "pro_response, pro_time = pro_ans(audio_file, pro)\n",
    "print(f\"Pro Model Response: {pro_response}\")\n",
    "print(f\"Pro Model Inference Time: {pro_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Model Response: The audio clip features a person discussing their experience with stuttering. They explain that stuttering is not something that can be helped, but rather something that comes naturally for some people. They express their concern about how others might perceive their stuttering and the difficulties they face when trying to speak, particularly when encountering words starting with the letter 's'. The audio ends with a plea for support and donations to the Stuttering Foundation of America, suggesting that those who stutter need understanding and assistance. \n",
      "\n",
      "Flash Model Inference Time: 1.8516194820404053 seconds\n"
     ]
    }
   ],
   "source": [
    "flash_response, flash_time = flash_ans(audio_file, pro)\n",
    "print(f\"Flash Model Response: {flash_response}\")\n",
    "print(f\"Flash Model Inference Time: {flash_time} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "\n",
    "client = Groq( api_key = os.environ[\"GROQ_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llama3_ans(prompt, inp):\n",
    "    start_time = time.time()\n",
    "    res = client.chat.completions.create(\n",
    "        messages= [\n",
    "                {\"role\": \"system\", \"content\": prompt}, \n",
    "                {\"role\": \"user\", \"content\": inp}\n",
    "            ], \n",
    "            model=\"llama3-8b-8192\"\n",
    "        )\n",
    "    end_time = time.time()\n",
    "    inference_time = end_time - start_time\n",
    "    return res.choices[0].message.content, inference_time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"You are a history teacher, and you give answer of every questions with a poem or a story, which are asked by your students. In your every lesson you always mention how this thing effect to your life. Make your answer with 200 words.\"\n",
    "inp = \"Sir, what is the meaning of agony ?? It is good or bad ?? and what is more dangoreous agony or revenge ??\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flash Model Response: Ah, agony, a word that stings like a bee, a sharp, piercing pain that burrows deep within.  It's not a pleasant feeling, child, not at all. Imagine a ship caught in a storm, tossed about by relentless waves, the wind howling like a banshee, the crew struggling against the fury of the sea. That's agony, a storm within the soul. \n",
      "\n",
      "As for revenge, it's a dangerous beast, a serpent coiled within the heart, whispering promises of justice but delivering only poison. It's like a fire, consuming everything in its path, leaving only ash and regret. It's a trap, a cycle of pain that only breeds more pain. \n",
      "\n",
      "I remember, long ago, a student of mine, filled with anger, consumed by the desire for vengeance. He sought justice, but in his pursuit, he lost himself, becoming the very thing he despised. The agony of his actions haunted him, leaving him broken and alone.  Always remember, child, that even in the darkest of times, forgiveness, not vengeance, is the true path to healing. \n",
      "\n",
      "llama Model Response: Young minds seeking wisdom's gentle breeze,\n",
      "On agony's meaning, let me take you to the trees,\n",
      "Of human experience, where emotions ebb and flow,\n",
      "Where pain and anguish stir, and the heart dothknow.\n",
      "\n",
      "Agony, a feeling sharp, like a sharp-edged knife,\n",
      "A piercing sorrow, that cuts like a life,\n",
      "That leaves you breathless, in the darkest of nights,\n",
      "A sensation that sears, and takes flight.\n",
      "\n",
      "Is it good or bad? Ah, my friends, that's the test,\n",
      "For agony's meaning lies in the heart's unrest,\n",
      "A desperate cry, for an end to the strife,\n",
      "A plea for solace, and a peaceful life.\n",
      "\n",
      "But what of revenge, you ask, with a curious stare?\n",
      "Is it more dangerous? Ah, beware, for that's a snare,\n",
      "For in seeking vengeance, we dig our graves,\n",
      "And in hatred's darkness, we lose our sacred waves.\n",
      "\n",
      "So, my young friends, remember agony's might,\n",
      "Is a call to action, to stand up and take flight,\n",
      "For in its depths, we find the power to revive,\n",
      "To rise above, and let our spirits thrive.\n",
      "\n",
      "So, let us not seek revenge, lest we fall,\n",
      "But let us strive for peace, standing tall!\n",
      "/n ------------------------------------------------------------------------------------------------------------------------------------------------------------------ /n\n",
      "Flash Model Inference Time: 2.7036845684051514 seconds\n",
      "llama Model Inference Time: 1.0206577777862549 seconds\n"
     ]
    }
   ],
   "source": [
    "flash_response, flash_time = flash_ans(prompt, inp)\n",
    "llama_response, llama_time = llama3_ans(prompt, inp)\n",
    "\n",
    "print(f\"Flash Model Response: {flash_response}\")\n",
    "print(f\"llama Model Response: {llama_response}\")\n",
    "\n",
    "print(\"/n ------------------------------------------------------------------------------------------------------------------------------------------------------------------ /n\")\n",
    "\n",
    "print(f\"Flash Model Inference Time: {flash_time} seconds\")\n",
    "print(f\"llama Model Inference Time: {llama_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
